---
title: "On the Terminology of Data Splits in Machine Learning"
date: 2025-08-30
category: Philosophy, thoughts and ramblings
layout: post
---

Recently I've decided to learn thoroughly the topic of data splits in machine learning (training/validation/testing, cross-validation, etc). First of all, during my PhD professors assumed everyone knew the topic (at least that's what I think in retrospect), which could very well be the case for most students. However, as my background was not in statistics, I learned the splits as I needed them. This meant I never took the time or effort to study the topic by itself (as I was focused on the intricacies of particular machine learning methods). Additionally, I never used or studied nested cross-validation, which is particularly useful in contexts such as low sample sizes.

We typically distinguish between training, validation, and test sets, each with its familiar role. However, not every situation needs all three sets. Before mentioning examples, it is important to say that there are two general approaches: the first consists in splitting the data into fixed sets, each taking a distinct "role" in the split. Alternatively, K-fold approaches recycle the data by alternating the roles of different subsets. Many details are omitted here (how large should each set be? how many folds should one use?), as the objective of this post is to discuss the basic terminology.

If all three sets are used, then the first approach mentioned is called a training/validation/test split. For the second approach, here’s where I see an issue with the terminology. If we are not evaluating our model on unseen data (meaning there are no testing sets), the procedure is called cross-validation. This makes perfect sense as the validation role is alternating between each of the K folds. However, when there is no hyperparameter selection (meaning there is no validation set), the resulting procedure is still called cross-validation. Finally, if all three "roles" are desired in the split, the resulting procedure is called a nested cross-validation, which again doesn’t align with the actual roles: training, validation, and testing, not training and validation twice. If the historical developments leading to the definition and popularization of these techniques were different, perhaps the names "cross-testing" and "cross-validation-testing", which are more aligned with what is being done (in the cases I mentioned above) would have been chosen.

Not only does the current nomenclature misrepresent what's actually being done by some splits, it also is inconsistent with the training/validation/testing pattern and related ones (training/validation and training/testing). Of course, there are no problems with the current notation if everyone understands the context in which it is being used. However, in a few cases where the term "cross-validation" is being used before context is introduced, confusion may arise (even if temporary). In those cases, a clearer terminology would certainly be helpful. At the very least, being explicit about whether cross-validation is used for hyperparameter selection, performance evaluation, or both might avoid temporary confusion.
